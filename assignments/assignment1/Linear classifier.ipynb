{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "      <th>3072</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.089754</td>\n",
       "      <td>-0.035684</td>\n",
       "      <td>0.089948</td>\n",
       "      <td>-0.093667</td>\n",
       "      <td>-0.040285</td>\n",
       "      <td>0.084531</td>\n",
       "      <td>-0.097578</td>\n",
       "      <td>-0.049002</td>\n",
       "      <td>0.074858</td>\n",
       "      <td>-0.109567</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046039</td>\n",
       "      <td>0.084531</td>\n",
       "      <td>-0.120953</td>\n",
       "      <td>-0.035040</td>\n",
       "      <td>0.100906</td>\n",
       "      <td>-0.126547</td>\n",
       "      <td>-0.039985</td>\n",
       "      <td>0.097104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.019218</td>\n",
       "      <td>0.054654</td>\n",
       "      <td>0.012215</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>0.049236</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>0.039564</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081333</td>\n",
       "      <td>-0.056645</td>\n",
       "      <td>-0.128796</td>\n",
       "      <td>-0.113472</td>\n",
       "      <td>-0.087329</td>\n",
       "      <td>-0.150077</td>\n",
       "      <td>-0.134103</td>\n",
       "      <td>-0.106818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.254460</td>\n",
       "      <td>-0.329802</td>\n",
       "      <td>-0.368875</td>\n",
       "      <td>-0.254452</td>\n",
       "      <td>-0.330481</td>\n",
       "      <td>-0.370371</td>\n",
       "      <td>-0.254441</td>\n",
       "      <td>-0.331355</td>\n",
       "      <td>-0.372201</td>\n",
       "      <td>-0.254665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265647</td>\n",
       "      <td>-0.280175</td>\n",
       "      <td>-0.187619</td>\n",
       "      <td>-0.235040</td>\n",
       "      <td>-0.252035</td>\n",
       "      <td>-0.169685</td>\n",
       "      <td>-0.204691</td>\n",
       "      <td>-0.232308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114168</td>\n",
       "      <td>0.160394</td>\n",
       "      <td>0.089948</td>\n",
       "      <td>0.212215</td>\n",
       "      <td>0.253833</td>\n",
       "      <td>0.174727</td>\n",
       "      <td>0.063206</td>\n",
       "      <td>0.052959</td>\n",
       "      <td>0.027799</td>\n",
       "      <td>-0.125253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232392</td>\n",
       "      <td>0.194335</td>\n",
       "      <td>0.181008</td>\n",
       "      <td>0.227705</td>\n",
       "      <td>0.191102</td>\n",
       "      <td>0.183256</td>\n",
       "      <td>0.230603</td>\n",
       "      <td>0.199065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125933</td>\n",
       "      <td>0.132943</td>\n",
       "      <td>0.066419</td>\n",
       "      <td>0.118097</td>\n",
       "      <td>0.124421</td>\n",
       "      <td>0.057080</td>\n",
       "      <td>0.118108</td>\n",
       "      <td>0.123547</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.117884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130431</td>\n",
       "      <td>0.088453</td>\n",
       "      <td>0.133949</td>\n",
       "      <td>0.133587</td>\n",
       "      <td>0.081298</td>\n",
       "      <td>0.124433</td>\n",
       "      <td>0.124720</td>\n",
       "      <td>0.077496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>-0.250538</td>\n",
       "      <td>-0.172939</td>\n",
       "      <td>-0.235542</td>\n",
       "      <td>-0.270138</td>\n",
       "      <td>-0.173618</td>\n",
       "      <td>-0.237038</td>\n",
       "      <td>-0.281892</td>\n",
       "      <td>-0.166649</td>\n",
       "      <td>-0.234946</td>\n",
       "      <td>-0.266429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036314</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>-0.050364</td>\n",
       "      <td>0.047313</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>-0.044195</td>\n",
       "      <td>0.058054</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>0.184756</td>\n",
       "      <td>0.191767</td>\n",
       "      <td>0.180144</td>\n",
       "      <td>0.172999</td>\n",
       "      <td>0.179323</td>\n",
       "      <td>0.166884</td>\n",
       "      <td>0.165167</td>\n",
       "      <td>0.166684</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.180629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040235</td>\n",
       "      <td>0.029629</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.066920</td>\n",
       "      <td>0.065612</td>\n",
       "      <td>0.089139</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>0.380835</td>\n",
       "      <td>0.368237</td>\n",
       "      <td>0.360536</td>\n",
       "      <td>0.420058</td>\n",
       "      <td>0.414617</td>\n",
       "      <td>0.398256</td>\n",
       "      <td>0.451441</td>\n",
       "      <td>0.452959</td>\n",
       "      <td>0.431721</td>\n",
       "      <td>0.427688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224549</td>\n",
       "      <td>0.210021</td>\n",
       "      <td>0.216302</td>\n",
       "      <td>0.231626</td>\n",
       "      <td>0.218553</td>\n",
       "      <td>0.214629</td>\n",
       "      <td>0.230603</td>\n",
       "      <td>0.218673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>-0.242695</td>\n",
       "      <td>-0.223920</td>\n",
       "      <td>-0.129660</td>\n",
       "      <td>-0.242687</td>\n",
       "      <td>-0.220677</td>\n",
       "      <td>-0.123313</td>\n",
       "      <td>-0.242676</td>\n",
       "      <td>-0.221551</td>\n",
       "      <td>-0.125142</td>\n",
       "      <td>-0.250743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163686</td>\n",
       "      <td>-0.154685</td>\n",
       "      <td>-0.132717</td>\n",
       "      <td>-0.156609</td>\n",
       "      <td>-0.146153</td>\n",
       "      <td>-0.114783</td>\n",
       "      <td>-0.141946</td>\n",
       "      <td>-0.130347</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.027061</td>\n",
       "      <td>0.187987</td>\n",
       "      <td>0.251431</td>\n",
       "      <td>0.273441</td>\n",
       "      <td>0.351197</td>\n",
       "      <td>0.329873</td>\n",
       "      <td>0.362763</td>\n",
       "      <td>0.365054</td>\n",
       "      <td>0.321806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044157</td>\n",
       "      <td>0.092374</td>\n",
       "      <td>-0.018992</td>\n",
       "      <td>-0.038962</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.093060</td>\n",
       "      <td>0.069818</td>\n",
       "      <td>0.108869</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 3074 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.089754 -0.035684  0.089948 -0.093667 -0.040285  0.084531 -0.097578   \n",
       "1     0.016129  0.019218  0.054654  0.012215  0.010696  0.049236  0.000461   \n",
       "2    -0.254460 -0.329802 -0.368875 -0.254452 -0.330481 -0.370371 -0.254441   \n",
       "3     0.114168  0.160394  0.089948  0.212215  0.253833  0.174727  0.063206   \n",
       "4     0.125933  0.132943  0.066419  0.118097  0.124421  0.057080  0.118108   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8995 -0.250538 -0.172939 -0.235542 -0.270138 -0.173618 -0.237038 -0.281892   \n",
       "8996  0.184756  0.191767  0.180144  0.172999  0.179323  0.166884  0.165167   \n",
       "8997  0.380835  0.368237  0.360536  0.420058  0.414617  0.398256  0.451441   \n",
       "8998 -0.242695 -0.223920 -0.129660 -0.242687 -0.220677 -0.123313 -0.242676   \n",
       "8999  0.012207  0.027061  0.187987  0.251431  0.273441  0.351197  0.329873   \n",
       "\n",
       "             7         8         9  ...      3064      3065      3066  \\\n",
       "0    -0.049002  0.074858 -0.109567  ... -0.046039  0.084531 -0.120953   \n",
       "1    -0.001943  0.039564 -0.027214  ... -0.081333 -0.056645 -0.128796   \n",
       "2    -0.331355 -0.372201 -0.254665  ... -0.265647 -0.280175 -0.187619   \n",
       "3     0.052959  0.027799 -0.125253  ...  0.232392  0.194335  0.181008   \n",
       "4     0.123547  0.055250  0.117884  ...  0.130431  0.088453  0.133949   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8995 -0.166649 -0.234946 -0.266429  ...  0.036314  0.002178 -0.050364   \n",
       "8996  0.166684  0.161133  0.180629  ...  0.040235  0.029629  0.075126   \n",
       "8997  0.452959  0.431721  0.427688  ...  0.224549  0.210021  0.216302   \n",
       "8998 -0.221551 -0.125142 -0.250743  ... -0.163686 -0.154685 -0.132717   \n",
       "8999  0.362763  0.365054  0.321806  ...  0.044157  0.092374 -0.018992   \n",
       "\n",
       "          3067      3068      3069      3070      3071  3072  y  \n",
       "0    -0.035040  0.100906 -0.126547 -0.039985  0.097104   1.0  9  \n",
       "1    -0.113472 -0.087329 -0.150077 -0.134103 -0.106818   1.0  9  \n",
       "2    -0.235040 -0.252035 -0.169685 -0.204691 -0.232308   1.0  2  \n",
       "3     0.227705  0.191102  0.183256  0.230603  0.199065   1.0  1  \n",
       "4     0.133587  0.081298  0.124433  0.124720  0.077496   1.0  7  \n",
       "...        ...       ...       ...       ...       ...   ... ..  \n",
       "8995  0.047313  0.014632 -0.044195  0.058054  0.030437   1.0  1  \n",
       "8996  0.066920  0.065612  0.089139  0.085505  0.089261   1.0  7  \n",
       "8997  0.231626  0.218553  0.214629  0.230603  0.218673   1.0  4  \n",
       "8998 -0.156609 -0.146153 -0.114783 -0.141946 -0.130347   1.0  2  \n",
       "8999 -0.038962  0.010710  0.093060  0.069818  0.108869   1.0  7  \n",
       "\n",
       "[9000 rows x 3074 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_X)\n",
    "train_df['y'] = train_y\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 -10  -5]\n",
      " [  0 -11  -5]\n",
      " [ -2  -1   0]]\n",
      "[[-15]\n",
      " [-16]\n",
      " [ -3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  0.66666667,  0.33333333],\n",
       "       [-0.        ,  0.6875    ,  0.3125    ],\n",
       "       [ 0.66666667,  0.33333333, -0.        ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds = np.array([[10, 0, 5],\n",
    "#                   [11, 0, 6],\n",
    "#                   [1, 2, 3]])\n",
    "\n",
    "# preds -= np.max(preds, axis=-1)[..., None]\n",
    "# print(preds)\n",
    "# denominator = np.sum(preds, axis=-1)[..., None]\n",
    "# print(denominator)\n",
    "# preds / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 82.530609\n",
      "Epoch 1, loss: 89.268281\n",
      "Epoch 2, loss: 86.261790\n",
      "Epoch 3, loss: 80.517171\n",
      "Epoch 4, loss: 84.441692\n",
      "Epoch 5, loss: 82.701363\n",
      "Epoch 6, loss: 80.453971\n",
      "Epoch 7, loss: 85.065395\n",
      "Epoch 8, loss: 83.985147\n",
      "Epoch 9, loss: 87.921253\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=0.001, batch_size=300, reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1193181c10>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.189\n",
      "Epoch 0, loss: 87.661245\n",
      "Epoch 10, loss: 89.801733\n",
      "Epoch 20, loss: 86.950352\n",
      "Epoch 30, loss: 90.190402\n",
      "Epoch 40, loss: 86.208589\n",
      "Epoch 50, loss: 89.484593\n",
      "Epoch 60, loss: 90.972414\n",
      "Epoch 70, loss: 86.814674\n",
      "Epoch 80, loss: 88.365122\n",
      "Epoch 90, loss: 84.533032\n",
      "Accuracy after training for 1000 epochs:  0.159\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 1000 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.001, REG=0.0001\n",
      "Epoch 10, loss: 83.983876\n",
      "Epoch 20, loss: 81.863251\n",
      "Epoch 30, loss: 86.123867\n",
      "Epoch 40, loss: 81.899523\n",
      "Epoch 50, loss: 77.732218\n",
      "Epoch 60, loss: 75.716447\n",
      "Epoch 70, loss: 78.282408\n",
      "Epoch 80, loss: 81.528457\n",
      "Epoch 90, loss: 82.204212\n",
      "Epoch 100, loss: 71.877113\n",
      "Epoch 110, loss: 76.661587\n",
      "Epoch 120, loss: 76.456842\n",
      "Epoch 130, loss: 71.773772\n",
      "Epoch 140, loss: 73.030013\n",
      "Epoch 150, loss: 74.371440\n",
      "Epoch 160, loss: 72.434901\n",
      "Epoch 170, loss: 72.518900\n",
      "Epoch 180, loss: 71.189721\n",
      "Epoch 190, loss: 71.615462\n",
      "Epoch 200, loss: 76.238169\n",
      "Val Accuracy: 0.21\n",
      "\n",
      "\n",
      "LR=0.001, REG=1e-05\n",
      "Epoch 10, loss: 83.198787\n",
      "Epoch 20, loss: 78.916223\n",
      "Epoch 30, loss: 79.455671\n",
      "Epoch 40, loss: 77.530992\n",
      "Epoch 50, loss: 76.307011\n",
      "Epoch 60, loss: 78.339838\n",
      "Epoch 70, loss: 75.122114\n",
      "Epoch 80, loss: 76.018421\n",
      "Epoch 90, loss: 79.510448\n",
      "Epoch 100, loss: 79.290129\n",
      "Epoch 110, loss: 81.211066\n",
      "Epoch 120, loss: 75.300544\n",
      "Epoch 130, loss: 71.495285\n",
      "Epoch 140, loss: 76.362785\n",
      "Epoch 150, loss: 76.282619\n",
      "Epoch 160, loss: 68.861837\n",
      "Epoch 170, loss: 72.790058\n",
      "Epoch 180, loss: 77.632627\n",
      "Epoch 190, loss: 74.321960\n",
      "Epoch 200, loss: 68.465259\n",
      "Val Accuracy: 0.219\n",
      "\n",
      "\n",
      "LR=0.001, REG=1e-06\n",
      "Epoch 10, loss: 79.723748\n",
      "Epoch 20, loss: 79.645597\n",
      "Epoch 30, loss: 77.196967\n",
      "Epoch 40, loss: 77.854711\n",
      "Epoch 50, loss: 79.385105\n",
      "Epoch 60, loss: 78.813492\n",
      "Epoch 70, loss: 79.582144\n",
      "Epoch 80, loss: 75.555805\n",
      "Epoch 90, loss: 76.083038\n",
      "Epoch 100, loss: 76.042749\n",
      "Epoch 110, loss: 73.001243\n",
      "Epoch 120, loss: 78.667169\n",
      "Epoch 130, loss: 78.281370\n",
      "Epoch 140, loss: 75.105166\n",
      "Epoch 150, loss: 74.175767\n",
      "Epoch 160, loss: 70.458289\n",
      "Epoch 170, loss: 70.610996\n",
      "Epoch 180, loss: 77.022212\n",
      "Epoch 190, loss: 75.373743\n",
      "Epoch 200, loss: 74.298840\n",
      "Val Accuracy: 0.217\n",
      "\n",
      "\n",
      "LR=0.0001, REG=0.0001\n",
      "Epoch 10, loss: 65.633159\n",
      "Epoch 20, loss: 64.579394\n",
      "Epoch 30, loss: 64.067205\n",
      "Epoch 40, loss: 63.693720\n",
      "Epoch 50, loss: 63.428474\n",
      "Epoch 60, loss: 63.223817\n",
      "Epoch 70, loss: 63.027335\n",
      "Epoch 80, loss: 62.886398\n",
      "Epoch 90, loss: 62.736886\n",
      "Epoch 100, loss: 62.596819\n",
      "Epoch 110, loss: 62.477909\n",
      "Epoch 120, loss: 62.384386\n",
      "Epoch 130, loss: 62.283146\n",
      "Epoch 140, loss: 62.191184\n",
      "Epoch 150, loss: 62.087023\n",
      "Epoch 160, loss: 61.999924\n",
      "Epoch 170, loss: 61.930208\n",
      "Epoch 180, loss: 61.862252\n",
      "Epoch 190, loss: 61.789608\n",
      "Epoch 200, loss: 61.726777\n",
      "Val Accuracy: 0.252\n",
      "\n",
      "\n",
      "LR=0.0001, REG=1e-05\n",
      "Epoch 10, loss: 65.608696\n",
      "Epoch 20, loss: 64.571570\n",
      "Epoch 30, loss: 64.056692\n",
      "Epoch 40, loss: 63.707713\n",
      "Epoch 50, loss: 63.431723\n",
      "Epoch 60, loss: 63.226531\n",
      "Epoch 70, loss: 63.017980\n",
      "Epoch 80, loss: 62.868794\n",
      "Epoch 90, loss: 62.720221\n",
      "Epoch 100, loss: 62.602914\n",
      "Epoch 110, loss: 62.476324\n",
      "Epoch 120, loss: 62.387802\n",
      "Epoch 130, loss: 62.293571\n",
      "Epoch 140, loss: 62.190605\n",
      "Epoch 150, loss: 62.108423\n",
      "Epoch 160, loss: 62.025529\n",
      "Epoch 170, loss: 61.938931\n",
      "Epoch 180, loss: 61.884607\n",
      "Epoch 190, loss: 61.787677\n",
      "Epoch 200, loss: 61.740538\n",
      "Val Accuracy: 0.25\n",
      "\n",
      "\n",
      "LR=0.0001, REG=1e-06\n",
      "Epoch 10, loss: 65.626508\n",
      "Epoch 20, loss: 64.577787\n",
      "Epoch 30, loss: 64.058290\n",
      "Epoch 40, loss: 63.691218\n",
      "Epoch 50, loss: 63.425642\n",
      "Epoch 60, loss: 63.217910\n",
      "Epoch 70, loss: 63.034593\n",
      "Epoch 80, loss: 62.872232\n",
      "Epoch 90, loss: 62.723682\n",
      "Epoch 100, loss: 62.606351\n",
      "Epoch 110, loss: 62.495458\n",
      "Epoch 120, loss: 62.387381\n",
      "Epoch 130, loss: 62.281161\n",
      "Epoch 140, loss: 62.181899\n",
      "Epoch 150, loss: 62.114678\n",
      "Epoch 160, loss: 62.019838\n",
      "Epoch 170, loss: 61.944793\n",
      "Epoch 180, loss: 61.864077\n",
      "Epoch 190, loss: 61.784948\n",
      "Epoch 200, loss: 61.728770\n",
      "Val Accuracy: 0.249\n",
      "\n",
      "\n",
      "LR=1e-05, REG=0.0001\n",
      "Epoch 10, loss: 68.381792\n",
      "Epoch 20, loss: 67.790067\n",
      "Epoch 30, loss: 67.302658\n",
      "Epoch 40, loss: 66.893723\n",
      "Epoch 50, loss: 66.547891\n",
      "Epoch 60, loss: 66.253395\n",
      "Epoch 70, loss: 66.000305\n",
      "Epoch 80, loss: 65.784239\n",
      "Epoch 90, loss: 65.594590\n",
      "Epoch 100, loss: 65.426964\n",
      "Epoch 110, loss: 65.279611\n",
      "Epoch 120, loss: 65.151117\n",
      "Epoch 130, loss: 65.031120\n",
      "Epoch 140, loss: 64.923928\n",
      "Epoch 150, loss: 64.826963\n",
      "Epoch 160, loss: 64.738238\n",
      "Epoch 170, loss: 64.655425\n",
      "Epoch 180, loss: 64.579272\n",
      "Epoch 190, loss: 64.507094\n",
      "Epoch 200, loss: 64.438865\n",
      "Val Accuracy: 0.229\n",
      "\n",
      "\n",
      "LR=1e-05, REG=1e-05\n",
      "Epoch 10, loss: 68.382323\n",
      "Epoch 20, loss: 67.791375\n",
      "Epoch 30, loss: 67.303499\n",
      "Epoch 40, loss: 66.892683\n",
      "Epoch 50, loss: 66.547246\n",
      "Epoch 60, loss: 66.252898\n",
      "Epoch 70, loss: 66.000988\n",
      "Epoch 80, loss: 65.782095\n",
      "Epoch 90, loss: 65.594180\n",
      "Epoch 100, loss: 65.427887\n",
      "Epoch 110, loss: 65.280809\n",
      "Epoch 120, loss: 65.150594\n",
      "Epoch 130, loss: 65.032601\n",
      "Epoch 140, loss: 64.924061\n",
      "Epoch 150, loss: 64.827089\n",
      "Epoch 160, loss: 64.737490\n",
      "Epoch 170, loss: 64.656237\n",
      "Epoch 180, loss: 64.578438\n",
      "Epoch 190, loss: 64.506536\n",
      "Epoch 200, loss: 64.440112\n",
      "Val Accuracy: 0.228\n",
      "\n",
      "\n",
      "LR=1e-05, REG=1e-06\n",
      "Epoch 10, loss: 68.379464\n",
      "Epoch 20, loss: 67.787876\n",
      "Epoch 30, loss: 67.298881\n",
      "Epoch 40, loss: 66.890411\n",
      "Epoch 50, loss: 66.546781\n",
      "Epoch 60, loss: 66.252167\n",
      "Epoch 70, loss: 65.998918\n",
      "Epoch 80, loss: 65.779922\n",
      "Epoch 90, loss: 65.592031\n",
      "Epoch 100, loss: 65.426909\n",
      "Epoch 110, loss: 65.279556\n",
      "Epoch 120, loss: 65.146800\n",
      "Epoch 130, loss: 65.030844\n",
      "Epoch 140, loss: 64.924724\n",
      "Epoch 150, loss: 64.826998\n",
      "Epoch 160, loss: 64.736837\n",
      "Epoch 170, loss: 64.653465\n",
      "Epoch 180, loss: 64.577347\n",
      "Epoch 190, loss: 64.505855\n",
      "Epoch 200, loss: 64.440507\n",
      "Val Accuracy: 0.228\n",
      "\n",
      "\n",
      "best validation accuracy achieved: 0.252000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        print(f'LR={lr}, REG={reg}')\n",
    "        clf = linear_classifer.LinearSoftmaxClassifier()\n",
    "        loss_history = clf.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=300, reg=reg)\n",
    "        pred = clf.predict(val_X)\n",
    "        val_accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print(f'Val Accuracy: {val_accuracy}\\n\\n')\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_classifier = clf\n",
    "            best_val_accuracy = val_accuracy\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.208000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "# clf1 = linear_classifer.LinearSoftmaxClassifier()\n",
    "# loss_history = clf1.fit(train_X, train_y, epochs=200, learning_rate=0.0001, batch_size=300, reg=0.0001)\n",
    "# test_pred = clf1.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
